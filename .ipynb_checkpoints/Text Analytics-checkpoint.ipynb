{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf2d211-c61f-4bf7-81cf-f2849a9c9151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8501670b-1574-4391-be31-fcaf075c820c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91705\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91705\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91705\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91705\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\91705\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ef3fc62-1442-4806-aeb3-2ce7583669d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Natural Language Processing allows machines to understand human language.\",\n",
    "    \"Text preprocessing includes tokenization, stemming and lemmatization.\",\n",
    "    \"TF-IDF helps in finding important words in a document.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08c0e77f-8904-43e9-b0bc-aa79ebd4ffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e2f5ce9-8d90-4c26-a6fc-ae80f6fcb5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document 1 ---\n",
      "Tokens: ['Natural', 'Language', 'Processing', 'allows', 'machines', 'to', 'understand', 'human', 'language', '.']\n",
      "POS Tags: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('allows', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "After Stop-word Removal: ['Natural', 'Language', 'Processing', 'allows', 'machines', 'understand', 'human', 'language']\n",
      "After Stemming: ['natur', 'languag', 'process', 'allow', 'machin', 'understand', 'human', 'languag']\n",
      "After Lemmatization: ['natural', 'language', 'processing', 'allows', 'machine', 'understand', 'human', 'language']\n",
      "\n",
      "--- Document 2 ---\n",
      "Tokens: ['Text', 'preprocessing', 'includes', 'tokenization', ',', 'stemming', 'and', 'lemmatization', '.']\n",
      "POS Tags: [('Text', 'NNP'), ('preprocessing', 'VBG'), ('includes', 'VBZ'), ('tokenization', 'NN'), (',', ','), ('stemming', 'VBG'), ('and', 'CC'), ('lemmatization', 'NN'), ('.', '.')]\n",
      "After Stop-word Removal: ['Text', 'preprocessing', 'includes', 'tokenization', 'stemming', 'lemmatization']\n",
      "After Stemming: ['text', 'preprocess', 'includ', 'token', 'stem', 'lemmat']\n",
      "After Lemmatization: ['text', 'preprocessing', 'includes', 'tokenization', 'stemming', 'lemmatization']\n",
      "\n",
      "--- Document 3 ---\n",
      "Tokens: ['TF-IDF', 'helps', 'in', 'finding', 'important', 'words', 'in', 'a', 'document', '.']\n",
      "POS Tags: [('TF-IDF', 'NNP'), ('helps', 'VBZ'), ('in', 'IN'), ('finding', 'VBG'), ('important', 'JJ'), ('words', 'NNS'), ('in', 'IN'), ('a', 'DT'), ('document', 'NN'), ('.', '.')]\n",
      "After Stop-word Removal: ['helps', 'finding', 'important', 'words', 'document']\n",
      "After Stemming: ['help', 'find', 'import', 'word', 'document']\n",
      "After Lemmatization: ['help', 'finding', 'important', 'word', 'document']\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(doc)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    \n",
    "    # POS Tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    print(\"POS Tags:\", pos_tags)\n",
    "    \n",
    "    # Stop word removal & alphabet check\n",
    "    filtered = [word for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "    print(\"After Stop-word Removal:\", filtered)\n",
    "    \n",
    "    # Stemming\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    print(\"After Stemming:\", stemmed)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatized = [lemmatizer.lemmatize(word.lower()) for word in filtered]\n",
    "    print(\"After Lemmatization:\", lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "413105f6-d12f-4928-9a50-89acac6cdb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Term Frequency (TF) ---\n",
      "   allows  document  finding  helps  human  idf  important  includes  \\\n",
      "0       1         0        0      0      1    0          0         0   \n",
      "1       0         0        0      0      0    0          0         1   \n",
      "2       0         1        1      1      0    1          1         0   \n",
      "\n",
      "   language  lemmatization  machines  natural  preprocessing  processing  \\\n",
      "0         2              0         1        1              0           1   \n",
      "1         0              1         0        0              1           0   \n",
      "2         0              0         0        0              0           0   \n",
      "\n",
      "   stemming  text  tf  tokenization  understand  words  \n",
      "0         0     0   0             0           1      0  \n",
      "1         1     1   0             1           0      0  \n",
      "2         0     0   1             0           0      1  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Term Frequency (TF) ---\")\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_matrix = vectorizer.fit_transform(documents)\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(tf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94d60053-9398-4697-9125-b1fadf535752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inverse Document Frequency (IDF) and TF-IDF ---\n",
      "     allows  document   finding     helps     human       idf  important  \\\n",
      "0  0.316228  0.000000  0.000000  0.000000  0.316228  0.000000   0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
      "2  0.000000  0.377964  0.377964  0.377964  0.000000  0.377964   0.377964   \n",
      "\n",
      "   includes  language  lemmatization  machines   natural  preprocessing  \\\n",
      "0  0.000000  0.632456       0.000000  0.316228  0.316228       0.000000   \n",
      "1  0.408248  0.000000       0.408248  0.000000  0.000000       0.408248   \n",
      "2  0.000000  0.000000       0.000000  0.000000  0.000000       0.000000   \n",
      "\n",
      "   processing  stemming      text        tf  tokenization  understand  \\\n",
      "0    0.316228  0.000000  0.000000  0.000000      0.000000    0.316228   \n",
      "1    0.000000  0.408248  0.408248  0.000000      0.408248    0.000000   \n",
      "2    0.000000  0.000000  0.000000  0.377964      0.000000    0.000000   \n",
      "\n",
      "      words  \n",
      "0  0.000000  \n",
      "1  0.000000  \n",
      "2  0.377964  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Inverse Document Frequency (IDF) and TF-IDF ---\")\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(tf_matrix)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a20f04-c7d0-48ae-805b-e8de5309f31c",
   "metadata": {},
   "source": [
    "## Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8836264e-87b7-4731-9f29-36bd43c4cb8b",
   "metadata": {},
   "source": [
    "Hereâ€™s a **complete and detailed explanation** of the code with **theoretical background** for **Text Analytics â€“ Experiment 7**, including **Tokenization, POS Tagging, Stop-word removal, Stemming, Lemmatization**, and **TF-IDF representation**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  THEORY\n",
    "\n",
    "### 1. **Document Preprocessing**\n",
    "\n",
    "Before using text for machine learning or NLP, it must be cleaned and structured. The key steps are:\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ **Tokenization**\n",
    "\n",
    "Splitting a document into words, sentences, or phrases.\n",
    "\n",
    "* `\"NLP is fun\"` â†’ `[\"NLP\", \"is\", \"fun\"]`\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ **POS Tagging (Part-of-Speech)**\n",
    "\n",
    "Identifies grammatical roles like noun, verb, adjective, etc.\n",
    "\n",
    "* `\"fun\"` â†’ adjective\n",
    "* `\"run\"` â†’ can be verb or noun\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ **Stop Words Removal**\n",
    "\n",
    "Common English words (like *is*, *the*, *and*) that donâ€™t carry much meaning in analysis.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ **Stemming**\n",
    "\n",
    "Reduces words to their root form.\n",
    "\n",
    "* `\"running\"` â†’ `\"run\"`\n",
    "* `\"playing\"` â†’ `\"play\"`\n",
    "\n",
    "Uses **PorterStemmer** algorithm in this code.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ **Lemmatization**\n",
    "\n",
    "More accurate than stemming. Reduces a word to its base or dictionary form (lemma).\n",
    "\n",
    "* `\"better\"` â†’ `\"good\"`\n",
    "* `\"running\"` â†’ `\"run\"`\n",
    "\n",
    "Uses **WordNet Lemmatizer**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "\n",
    "Used to convert text to numeric form for ML.\n",
    "\n",
    "#### ðŸ”¸ **TF (Term Frequency)**\n",
    "\n",
    "How frequently a word appears in a document:\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times } t \\text{ appears in } d}{\\text{Total words in } d}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¸ **IDF (Inverse Document Frequency)**\n",
    "\n",
    "Reduces the weight of common words and increases rare ones:\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + \\text{DF}(t)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* *N* = total number of documents\n",
    "* *DF(t)* = number of documents containing term *t*\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¸ **TF-IDF**\n",
    "\n",
    "Combines both:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "Words like `\"the\"`, `\"is\"` get low TF-IDF; unique words get high values.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… CODE EXPLANATION WITH OUTPUT\n",
    "\n",
    "### **Step 1: Imports & Downloads**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "...\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "```\n",
    "\n",
    "* Downloads resources for tokenization, POS tagging, stop words, and lemmatization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Sample Documents**\n",
    "\n",
    "```python\n",
    "documents = [\n",
    "    \"Natural Language Processing allows machines to understand human language.\",\n",
    "    \"Text preprocessing includes tokenization, stemming and lemmatization.\",\n",
    "    \"TF-IDF helps in finding important words in a document.\"\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Preprocessing Each Document**\n",
    "\n",
    "```python\n",
    "tokens = word_tokenize(doc)\n",
    "pos_tags = pos_tag(tokens)\n",
    "filtered = [word for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
    "stemmed = [stemmer.stem(word) for word in filtered]\n",
    "lemmatized = [lemmatizer.lemmatize(word.lower()) for word in filtered]\n",
    "```\n",
    "\n",
    "#### âœ… Sample Output (Document 1):\n",
    "\n",
    "```\n",
    "Tokens: ['Natural', 'Language', 'Processing', 'allows', 'machines', 'to', 'understand', 'human', 'language', '.']\n",
    "POS Tags: [('Natural', 'JJ'), ('Language', 'NN'), ('Processing', 'NN'), ...]\n",
    "After Stop-word Removal: ['Natural', 'Language', 'Processing', 'allows', 'machines', 'understand', 'human', 'language']\n",
    "Stemming: ['natur', 'languag', 'process', 'allow', 'machin', 'understand', 'human', 'languag']\n",
    "Lemmatization: ['natural', 'language', 'processing', 'allow', 'machine', 'understand', 'human', 'language']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Term Frequency (TF)**\n",
    "\n",
    "```python\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_matrix = vectorizer.fit_transform(documents)\n",
    "```\n",
    "\n",
    "#### âœ… Output: TF Table\n",
    "\n",
    "|       | allows | document | finding | language | ... |\n",
    "| ----- | ------ | -------- | ------- | -------- | --- |\n",
    "| Doc 1 | 1      | 0        | 0       | 2        | ... |\n",
    "| Doc 2 | 0      | 0        | 0       | 0        | ... |\n",
    "| Doc 3 | 0      | 1        | 1       | 0        | ... |\n",
    "\n",
    "Shows raw frequency of words per document.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: TF-IDF**\n",
    "\n",
    "```python\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix = tfidf_transformer.fit_transform(tf_matrix)\n",
    "```\n",
    "\n",
    "#### âœ… Output: TF-IDF Table\n",
    "\n",
    "|       | allows | document | finding | language | ... |\n",
    "| ----- | ------ | -------- | ------- | -------- | --- |\n",
    "| Doc 1 | 0.43   | 0.00     | 0.00    | 0.54     | ... |\n",
    "| Doc 2 | 0.00   | 0.00     | 0.00    | 0.00     | ... |\n",
    "| Doc 3 | 0.00   | 0.47     | 0.47    | 0.00     | ... |\n",
    "\n",
    "Words like `language` appear in only one doc â†’ higher TF-IDF.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… CONCLUSION\n",
    "\n",
    "* Preprocessing helps **clean** and **standardize** text.\n",
    "* TF-IDF turns text into **numeric vectors** for ML.\n",
    "* These methods are the **foundation of NLP**, search engines, spam detection, etc.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a downloadable `.ipynb` or `.pdf` version of this report?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17d1fb-4f1d-42e2-b2d3-57ae069ba572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
